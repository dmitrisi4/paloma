// Создайте новый файл: src/services/ai/geminiService.ts
import { GoogleGenerativeAI, GenerativeModel } from '@google/generative-ai';
import logger from '../../utils/logger';

// Получаем API ключ из переменных окружения
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

// Проверка наличия API ключа
if (!GEMINI_API_KEY) {
	logger.error('Missing GEMINI_API_KEY in environment variables');
	throw new Error('Missing GEMINI_API_KEY in environment variables');
}

// Инициализация клиента Gemini
const genAI = new GoogleGenerativeAI(GEMINI_API_KEY);

// Модель по умолчанию (gemini-1.5-flash-latest)
const DEFAULT_MODEL = 'gemini-1.5-flash-latest';


export type MessageType = Array<{ role: 'system' | 'user' | 'assistant'; content: string }>;
export type OptionsType = {
	temperature?: number;
	maxOutputTokens?: number;
	model?: string;
};

/**
 * Отправляет запрос к Gemini API для генерации текста
 * @param messages Массив сообщений для контекста
 * @param options Дополнительные параметры для API
 * @returns Сгенерированный текст
 */
export const generateText = async (
	messages: MessageType,
	options: OptionsType = {}
): Promise<string | undefined> => {
	const finalOptions = {
		temperature: 0.7,
		maxOutputTokens: 500,
		model: DEFAULT_MODEL,
		...options,
	  };

	  
	logger.error(`OK - ${'1s'} - ${JSON.stringify(finalOptions)}`);

	if (!finalOptions.model) return;
	logger.error(`OK - ${'2s'}`);
	try {
		// Инициализация модели
		const model = genAI.getGenerativeModel({
			model: finalOptions.model,
			generationConfig: {
				temperature: finalOptions.temperature,
				maxOutputTokens: finalOptions.maxOutputTokens,
			},
		});

		// Преобразование формата сообщений из OpenAI в формат Gemini
		// Примечание: Gemini не имеет прямого эквивалента для system message
		// поэтому мы объединяем system message с первым user message
		let geminiMessages = [];
		let systemPrompt = '';

		// Ищем system prompt
		const systemMessage = messages.find(msg => msg.role === 'system');
		if (systemMessage) {
			systemPrompt = systemMessage.content;
		}

		// Создаем массив сообщений для Gemini
		messages.forEach(message => {
			if (message.role !== 'system') {
				const content = message.role === 'user' && geminiMessages.length === 0 && systemPrompt
					? `${systemPrompt}\n\n${message.content}` // Добавляем system prompt к первому user message
					: message.content;

				geminiMessages.push({
					role: message.role === 'assistant' ? 'model' : 'user',
					parts: [{ text: content }],
				});
			}
		});

		// Если у нас есть только system message, создаем пустое user message
		if (geminiMessages.length === 0 && systemPrompt) {
			geminiMessages.push({
				role: 'user',
				parts: [{ text: systemPrompt }],
			});
		}

		// Создаем или получаем существующий чат
		const chat = model.startChat({
			history: geminiMessages.slice(0, -1), // Исключаем последнее сообщение
		});

		// Отправляем последнее сообщение (или первое, если история пуста)
		const lastMessage = geminiMessages.length > 0
			? geminiMessages[geminiMessages.length - 1]
			: { role: 'user', parts: [{ text: 'Hello' }] };

		const result = await chat.sendMessage(lastMessage.parts[0].text);
		const response = result.response;
		const generatedText = response.text();

		if (!generatedText) {
			throw new Error('No content generated by Gemini');
		}

		return generatedText;
	} catch (error: any) {
		if (error.message?.includes("429")) {
			logger.error(
				"Rate limit reached or no free tier for the model. Consider downgrading the model or enabling billing."
			  );
		}
		logger.error('Error in Gemini API request:', error);
		throw new Error(`Failed to generate content: ${error?.message || 'Unknown error'}`);
	}
};